{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [https://github.com/milesial/Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "# import modules\n",
    "from models.networks.UNet import UNet\n",
    "from models.dataset.data_loading import BasicDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  epochs = 5                  # Number of epochs\n",
    "  batch_size = 1              # Batch size\n",
    "  learning_rate = 0.00001     # Learning rate\n",
    "  load = None                 # Load model from a .pth file (path)\n",
    "  scale = 0.5                 # Downscaling factor of the images\n",
    "  validation = 10.0           # Percent of the data that is used as validation (0-100)\n",
    "  amp = False                 # Use mixed precision\n",
    "  wandb = True                # toggle the usage of wandb for logging purposes \n",
    "  save = False                # save trained model\n",
    "  dataset_path = Path(\"../../resources/images/calibrated/dataset_1.npz\")\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net: UNet, dataloader: DataLoader, device: torch.device):\n",
    "    net.eval()\n",
    "    num_val_batches = len(dataloader)\n",
    "    loss = 0\n",
    "\n",
    "    # iterate over the validation set\n",
    "    for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
    "        image, mask_true = batch['image'], batch['mask']\n",
    "        # move images and labels to correct device and type\n",
    "        image = image.to(device=device, dtype=torch.float32)\n",
    "        mask_true = mask_true.to(device=device, dtype=torch.float32)            # NxWxH\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # predict the mask\n",
    "            mask_pred = net(image)\n",
    "            loss += torch.abs(mask_pred - mask_true).sum()\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Fixes a potential division by zero error\n",
    "    if num_val_batches == 0:\n",
    "        return loss \n",
    "\n",
    "    return loss / num_val_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net: UNet,\n",
    "              device: torch.device,\n",
    "              epochs: int = 5,\n",
    "              batch_size: int = 1,\n",
    "              learning_rate: float = 0.001,\n",
    "              val_percent: float = 0.1,\n",
    "              save_checkpoint: bool = True,\n",
    "              img_scale: float = 0.5,\n",
    "              amp: bool = False):\n",
    "    # 1. Create dataset\n",
    "    dataset = BasicDataset(args.dataset_path, img_scale)\n",
    "\n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    # TODO: check if generator needed!\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "    # (Initialize logging)\n",
    "    if args.wandb:\n",
    "        experiment = wandb.init(project='U-Net', resume='allow', entity=\"depth-denoising\")\n",
    "        experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                    val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale,\n",
    "                                    amp=amp))\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "        ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.L1Loss()\n",
    "    global_step = 0\n",
    "\n",
    "    # 5. Begin training\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "\n",
    "                assert images.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    masks_pred = net(images)\n",
    "                    loss = criterion(masks_pred, true_masks)\n",
    "                    \"\"\"\n",
    "                    loss = criterion(masks_pred, true_masks) \\\n",
    "                           + dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
    "                                       F.one_hot(true_masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                                       multiclass=True)\n",
    "                    \"\"\"\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if args.wandb:\n",
    "                    experiment.log({\n",
    "                        'train loss': loss.item(),\n",
    "                        'step': global_step,\n",
    "                        'epoch': epoch\n",
    "                    })\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # Evaluation round\n",
    "                division_step = (n_train // (10 * batch_size))\n",
    "                if division_step > 0 and global_step % division_step == 0:\n",
    "                    val_loss = evaluate(net, val_loader, device)\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                    logging.info('Validation Loss: {}'.format(val_loss))\n",
    "\n",
    "                    if args.wandb:\n",
    "                        histograms = {}\n",
    "                        for tag, value in net.named_parameters():\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        experiment.log({\n",
    "                            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                            'validation loss': val_loss,\n",
    "                            'images': wandb.Image(images[0].cpu()),\n",
    "                            'masks': {\n",
    "                                'true': wandb.Image(true_masks[0].float().cpu()),\n",
    "                                'pred': wandb.Image(masks_pred[0].float().cpu()),\n",
    "                            },\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch,\n",
    "                            **histograms\n",
    "                        })\n",
    "\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch + 1)))\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cpu\n",
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t1 output channels (classes)\n",
      "\tBilinear upscaling\n",
      "INFO: Creating dataset with 24 examples\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "/home/claudiusk/.conda/envs/depth-denoising_training/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/depth-denoising/U-Net/runs/1dx8jmqc\" target=\"_blank\">electric-durian-9</a></strong> to <a href=\"https://wandb.ai/depth-denoising/U-Net\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "        Epochs:          5\n",
      "        Batch size:      1\n",
      "        Learning rate:   1e-05\n",
      "        Training size:   22\n",
      "        Validation size: 2\n",
      "        Checkpoints:     False\n",
      "        Device:          cpu\n",
      "        Images scaling:  0.5\n",
      "        Mixed Precision: False\n",
      "        \n",
      "Epoch 1/5:   0%|          | 0/22 [00:00<?, ?img/s]/home/claudiusk/.conda/envs/depth-denoising_training/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "Epoch 1/5:   9%|▉         | 2/22 [00:31<05:01, 15.07s/img, loss (batch)=0.429]INFO: Validation Loss: 226741.71875\n",
      "Epoch 1/5:  18%|█▊        | 4/22 [01:08<05:05, 16.97s/img, loss (batch)=0.537]INFO: Validation Loss: 221434.65625\n",
      "Epoch 1/5:  27%|██▋       | 6/22 [01:43<04:29, 16.87s/img, loss (batch)=0.454]INFO: Validation Loss: 218997.84375\n",
      "Epoch 1/5:  36%|███▋      | 8/22 [02:19<04:02, 17.30s/img, loss (batch)=0.586]INFO: Validation Loss: 216996.0\n",
      "Epoch 1/5:  45%|████▌     | 10/22 [02:55<03:27, 17.26s/img, loss (batch)=0.325]INFO: Validation Loss: 218953.140625\n",
      "Epoch 1/5:  55%|█████▍    | 12/22 [03:31<02:53, 17.32s/img, loss (batch)=0.372]INFO: Validation Loss: 217958.625\n",
      "Epoch 1/5:  64%|██████▎   | 14/22 [04:06<02:15, 16.98s/img, loss (batch)=0.277]INFO: Validation Loss: 219761.3125\n",
      "Epoch 1/5:  73%|███████▎  | 16/22 [04:37<01:36, 16.07s/img, loss (batch)=0.252]INFO: Validation Loss: 215656.09375\n",
      "Epoch 1/5:  82%|████████▏ | 18/22 [05:09<01:02, 15.56s/img, loss (batch)=0.348]INFO: Validation Loss: 212443.34375\n",
      "Epoch 1/5:  91%|█████████ | 20/22 [05:41<00:30, 15.33s/img, loss (batch)=0.3]INFO: Validation Loss: 206152.34375\n",
      "Epoch 1/5: 100%|██████████| 22/22 [06:13<00:00, 15.45s/img, loss (batch)=0.285]INFO: Validation Loss: 202315.46875\n",
      "Epoch 1/5: 100%|██████████| 22/22 [06:20<00:00, 17.28s/img, loss (batch)=0.285]\n",
      "Epoch 2/5:   9%|▉         | 2/22 [00:39<06:37, 19.89s/img, loss (batch)=0.212]INFO: Validation Loss: 193245.609375\n",
      "Epoch 2/5:  18%|█▊        | 4/22 [01:23<06:12, 20.70s/img, loss (batch)=0.24] INFO: Validation Loss: 184645.03125\n",
      "Epoch 2/5:  27%|██▋       | 6/22 [02:09<05:47, 21.70s/img, loss (batch)=0.276]INFO: Validation Loss: 176371.90625\n",
      "Epoch 2/5:  36%|███▋      | 8/22 [02:51<04:54, 21.05s/img, loss (batch)=0.197]INFO: Validation Loss: 170977.53125\n",
      "Epoch 2/5:  45%|████▌     | 10/22 [03:31<04:03, 20.32s/img, loss (batch)=0.246]INFO: Validation Loss: 163080.203125\n",
      "Epoch 2/5:  55%|█████▍    | 12/22 [04:14<03:25, 20.52s/img, loss (batch)=0.346]INFO: Validation Loss: 151802.140625\n",
      "Epoch 2/5:  64%|██████▎   | 14/22 [04:53<02:36, 19.53s/img, loss (batch)=0.284]INFO: Validation Loss: 151231.84375\n",
      "Epoch 2/5:  73%|███████▎  | 16/22 [05:33<01:57, 19.53s/img, loss (batch)=0.273]INFO: Validation Loss: 150895.8125\n",
      "Epoch 2/5:  82%|████████▏ | 18/22 [06:14<01:18, 19.66s/img, loss (batch)=0.468]INFO: Validation Loss: 148353.984375\n",
      "Epoch 2/5:  91%|█████████ | 20/22 [06:50<00:37, 18.65s/img, loss (batch)=0.235]INFO: Validation Loss: 150867.59375\n",
      "Epoch 2/5: 100%|██████████| 22/22 [07:28<00:00, 18.24s/img, loss (batch)=0.195]INFO: Validation Loss: 151305.65625\n",
      "Epoch 2/5: 100%|██████████| 22/22 [07:35<00:00, 20.71s/img, loss (batch)=0.195]\n",
      "Epoch 3/5:   9%|▉         | 2/22 [00:31<05:17, 15.87s/img, loss (batch)=0.196]INFO: Validation Loss: 142684.953125\n",
      "Epoch 3/5:  18%|█▊        | 4/22 [01:10<05:22, 17.93s/img, loss (batch)=0.198]INFO: Validation Loss: 141028.78125\n",
      "Epoch 3/5:  27%|██▋       | 6/22 [01:52<05:04, 19.02s/img, loss (batch)=0.295]INFO: Validation Loss: 142617.375\n",
      "Epoch 3/5:  36%|███▋      | 8/22 [02:34<04:36, 19.74s/img, loss (batch)=0.188]INFO: Validation Loss: 140347.53125\n",
      "Epoch 3/5:  45%|████▌     | 10/22 [03:15<03:57, 19.75s/img, loss (batch)=0.274]INFO: Validation Loss: 141853.25\n",
      "Epoch 3/5:  55%|█████▍    | 12/22 [03:53<03:10, 19.00s/img, loss (batch)=0.282]INFO: Validation Loss: 143369.46875\n",
      "Epoch 3/5:  64%|██████▎   | 14/22 [04:33<02:32, 19.08s/img, loss (batch)=0.244]INFO: Validation Loss: 145563.5625\n",
      "Epoch 3/5:  73%|███████▎  | 16/22 [05:11<01:52, 18.76s/img, loss (batch)=0.234]INFO: Validation Loss: 144799.953125\n",
      "Epoch 3/5:  82%|████████▏ | 18/22 [05:56<01:21, 20.47s/img, loss (batch)=0.256]INFO: Validation Loss: 145659.28125\n",
      "Epoch 3/5:  91%|█████████ | 20/22 [06:33<00:38, 19.13s/img, loss (batch)=0.21] INFO: Validation Loss: 145571.28125\n",
      "Epoch 3/5: 100%|██████████| 22/22 [07:14<00:00, 19.47s/img, loss (batch)=0.359]INFO: Validation Loss: 142738.109375\n",
      "Epoch 3/5: 100%|██████████| 22/22 [07:22<00:00, 20.11s/img, loss (batch)=0.359]\n",
      "Epoch 4/5:   9%|▉         | 2/22 [00:29<04:52, 14.61s/img, loss (batch)=0.345]INFO: Validation Loss: 140950.75\n",
      "Epoch 4/5:  18%|█▊        | 4/22 [01:08<05:13, 17.43s/img, loss (batch)=0.271]INFO: Validation Loss: 144737.625\n",
      "Epoch 4/5:  27%|██▋       | 6/22 [01:50<05:04, 19.03s/img, loss (batch)=0.273]INFO: Validation Loss: 141741.6875\n",
      "Epoch 4/5:  36%|███▋      | 8/22 [02:29<04:24, 18.91s/img, loss (batch)=0.285]INFO: Validation Loss: 145950.453125\n",
      "Epoch 4/5:  45%|████▌     | 10/22 [03:12<03:58, 19.89s/img, loss (batch)=0.243]INFO: Validation Loss: 144500.984375\n",
      "Epoch 4/5:  55%|█████▍    | 12/22 [03:52<03:14, 19.50s/img, loss (batch)=0.278]INFO: Validation Loss: 145908.59375\n",
      "Epoch 4/5:  64%|██████▎   | 14/22 [04:32<02:37, 19.67s/img, loss (batch)=0.196]INFO: Validation Loss: 145860.890625\n",
      "Epoch 4/5:  73%|███████▎  | 16/22 [05:16<02:02, 20.39s/img, loss (batch)=0.254]INFO: Validation Loss: 152334.265625\n",
      "Epoch 4/5:  82%|████████▏ | 18/22 [05:56<01:18, 19.68s/img, loss (batch)=0.187]INFO: Validation Loss: 155336.609375\n",
      "Epoch 4/5:  91%|█████████ | 20/22 [06:39<00:40, 20.30s/img, loss (batch)=0.49] INFO: Validation Loss: 147306.75\n",
      "Epoch 4/5: 100%|██████████| 22/22 [07:24<00:00, 21.33s/img, loss (batch)=0.294]INFO: Validation Loss: 150529.84375\n",
      "Epoch 4/5: 100%|██████████| 22/22 [07:32<00:00, 20.57s/img, loss (batch)=0.294]\n",
      "Epoch 5/5:   9%|▉         | 2/22 [00:29<04:59, 14.96s/img, loss (batch)=0.281]INFO: Validation Loss: 157145.3125\n",
      "Epoch 5/5:  18%|█▊        | 4/22 [01:11<05:34, 18.61s/img, loss (batch)=0.284]INFO: Validation Loss: 157772.609375\n",
      "Epoch 5/5:  27%|██▋       | 6/22 [02:00<05:46, 21.66s/img, loss (batch)=0.193]INFO: Validation Loss: 162772.0\n",
      "Epoch 5/5:  36%|███▋      | 8/22 [02:45<05:05, 21.79s/img, loss (batch)=0.253]INFO: Validation Loss: 165186.890625\n",
      "Epoch 5/5:  45%|████▌     | 10/22 [03:28<04:15, 21.26s/img, loss (batch)=0.241]INFO: Validation Loss: 167636.859375\n",
      "Epoch 5/5:  50%|█████     | 11/22 [03:54<04:10, 22.76s/img, loss (batch)=0.192]"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')\n",
    "\n",
    "# Change here to adapt to your data\n",
    "# n_channels=3 for RGB images\n",
    "# n_classes is the number of probabilities you want to get per pixel\n",
    "net = UNet(n_channels=1, n_classes=1, bilinear=True)\n",
    "\n",
    "logging.info(f'Network:\\n'\n",
    "              f'\\t{net.n_channels} input channels\\n'\n",
    "              f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "              f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "if args.load:\n",
    "    net.load_state_dict(torch.load(args.load, map_location=device))\n",
    "    logging.info(f'Model loaded from {args.load}')\n",
    "\n",
    "net.to(device=device)\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              epochs=args.epochs,\n",
    "              batch_size=args.batch_size,\n",
    "              learning_rate=args.learning_rate,\n",
    "              device=device,\n",
    "              img_scale=args.scale,\n",
    "              save_checkpoint=args.save,\n",
    "              val_percent=args.validation / 100,\n",
    "              amp=args.amp)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a16bc2ce7b7f1639c4a57155c0fb5a1be6acb415dada3521f659853ec55ce1a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('depth-denoising': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
